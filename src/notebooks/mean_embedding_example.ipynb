{"cells":[{"cell_type":"code","execution_count":1,"id":"dcf681af-06e1-4a6e-bf96-7538fb66bdea","metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms, models"]},{"cell_type":"code","execution_count":2,"id":"9658e9b0-10b4-49ef-ad2a-bad412d8015d","metadata":{},"outputs":[],"source":["import random\n","from PIL import ImageFilter\n","\n","class GaussianBlur(object):\n","    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n","\n","    def __init__(self, sigma=[.1, 2.]):\n","        self.sigma = sigma\n","\n","    def __call__(self, x):\n","        sigma = random.uniform(self.sigma[0], self.sigma[1])\n","        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n","        return x"]},{"cell_type":"code","execution_count":3,"id":"40375773-7c69-472d-ab5c-d07bfd8c1536","metadata":{},"outputs":[],"source":["class UnNormalize(object):\n","    def __init__(self, mean, std):\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor):\n","        \"\"\"\n","        Args:\n","            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n","        Returns:\n","            Tensor: Normalized image.\n","        \"\"\"\n","        for t, m, s in zip(tensor, self.mean, self.std):\n","            t.mul_(s).add_(m)\n","            # The normalize code -> t.sub_(m).div_(s)\n","        return tensor"]},{"cell_type":"code","execution_count":15,"id":"d939e291-9459-4b57-958d-6f9846768ab3","metadata":{"trusted":true},"outputs":[],"source":["class MeanEmbeddingModel(nn.Module):\n","    def __init__(self, backbone, mode, k=100, image_shape=(3, 224, 224), feature_dim=2048, device='cpu'):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.mode = mode\n","        self.k = k\n","        self.image_shape = image_shape\n","        self.feature_dim = feature_dim\n","        self.device = device\n","\n","        if self.mode == 'ventral':\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor()\n","            ])\n","        elif self.mode == 'dorsal':\n","            self.transform = transforms.Compose([\n","                UnNormalize(*imagenet_mean_std),\n","                transforms.ToPILImage(),\n","                transforms.RandomApply([\n","                    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n","                ], p=0.8),\n","                transforms.RandomGrayscale(p=0.2),\n","                transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n","                transforms.ToTensor(),\n","                transforms.Normalize(*imagenet_mean_std)\n","            ])\n","\n","    def forward(self, x):\n","        # prepare the output data structure\n","        mean_embedding = torch.zeros((x.shape[0], self.feature_dim))\n","        # for each image in the batch x.\n","        for j in range(len(x)):\n","            # prepare the data tensor that will hold the features of the augmentations of image x[j].\n","            t_x = torch.zeros((self.k, *self.image_shape))\n","            # augment image x[j] k times.\n","            for i in range(self.k):\n","                t_x[i, :] = self.transform(x[j].cpu())\n","            # pass these augmentations through the backbone.\n","            z = self.backbone(t_x.to(self.device))\n","            # take the mean of these vectors to create a single feature vector for image x[j]\n","            mean_embedding[j, :] = z.mean(dim=0)\n","        # return the mean embeddings for all images in the batch\n","        return mean_embedding"]},{"cell_type":"markdown","id":"9d2fbdbd-d279-450e-8514-8fa22b7fd635","metadata":{},"source":["### Setup the hyperparameters and a random batch of data"]},{"cell_type":"code","execution_count":16,"id":"c8fa944f-de11-48d2-8122-2f2af57c9159","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n","torch.Size([256, 3, 224, 224])\n"]}],"source":["#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = 'cpu'\n","print(device)\n","\n","batch_size = 256\n","imagenet_mean_std = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n","\n","data = torch.randn((batch_size, 3, 224, 224)).to(device=device)\n","print(data.shape)"]},{"cell_type":"markdown","id":"c9419938-6456-4e0f-b291-42bb18234c1d","metadata":{},"source":["### Create our mean embedding model with a ResNet50 backbone"]},{"cell_type":"code","execution_count":17,"id":"797fb5f9-18ae-49fe-8b99-134e42545fad","metadata":{},"outputs":[],"source":["model = MeanEmbeddingModel(models.resnet50(), mode='ventral', k=32, device='cpu')"]},{"cell_type":"markdown","id":"f8de683a-b2e0-424c-9bb5-dd99540c1264","metadata":{"tags":[]},"source":["### Pass our data through the model\n","* This will run the forward pass which augments each image within the batch k=32 times.\n","* Each augmented image is passed through the backbone.\n","* Then all the feature vectors that come from augmentations of the same image are averaged into a single feature vector -- the mean embedding.\n","* The output is therefore one mean embedding per image in the batch."]},{"cell_type":"code","execution_count":18,"id":"a306c228-22ff-49b6-9375-d02e387c0e5c","metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"The expanded size of the tensor (2048) must match the existing size (1000) at non-singleton dimension 0.  Target sizes: [2048].  Tensor sizes: [1000]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33376/355013827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmean_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/pt2/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33376/1019503199.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# take the mean of these vectors to create a single feature vector for image x[j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mmean_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# return the mean embeddings for all images in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2048) must match the existing size (1000) at non-singleton dimension 0.  Target sizes: [2048].  Tensor sizes: [1000]"]}],"source":["mean_embedding = model(data)\n","mean_embedding.shape"]},{"cell_type":"code","execution_count":null,"id":"df44044c-ad3d-4aab-9582-7b5d2d3fc4a5","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
