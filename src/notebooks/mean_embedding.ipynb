{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from pl_bolts.models.self_supervised import SimCLR\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:13, 13034076.91it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = CIFAR10('./', download=True,\n",
    "transform=transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "]), train=True)\n",
    "\n",
    "\n",
    "test_set = CIFAR10('./', download=True,\n",
    "transform=transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "]), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_image = images[0]\n",
    "dummy_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to run the model in evaluation mode. For now, implement a single image format where we have a single image and a set of transformations and we calculate the mean embeddings in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(LightningModule):\n",
    "    def __init__(self, encoder='resnet50_supervised'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "\n",
    "        #TODO: List of encoders in the configuration file\n",
    "        if encoder not in ['resnet50_supervised', 'simclr_r50', 'vit_base_patch8_224', 'vit_base_patch16_224_in21k', 'vit_base_patch32_224_in21k']:\n",
    "            raise AssertionError(\"Encoder not in the list of supported encoders.\")\n",
    "        \n",
    "        \n",
    "        if(self.encoder == 'resnet50_supervised'):\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            num_filters = backbone.fc.in_features\n",
    "            layers = list(backbone.children())[:-1]\n",
    "            self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        elif(self.encoder == 'simclr_r50'):\n",
    "            weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'\n",
    "            simclr = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "            self.feature_extractor = simclr.encoder\n",
    "\n",
    "        elif(self.encoder == 'vit_base_patch16_224_in21k'):\n",
    "            self.feature_extractor = timm.create_model('vit_base_patch16_224_in21k', pretrained=True, num_classes=0)\n",
    "            config = resolve_data_config({}, model=model)\n",
    "            transform = create_transform(**config)\n",
    "\n",
    "        elif(self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "            self.feature_extractor = timm.create_model('vit_base_patch32_224_in21k', pretrained=True, num_classes=0)\n",
    "            config = resolve_data_config({}, model=model)\n",
    "            transform = create_transform(**config)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            if(self.encoder == 'resnet50_supervised'):\n",
    "                representations = self.feature_extractor(x).flatten(1)\n",
    "            elif(self.encoder == 'simclr_r50'):\n",
    "                representations = self.feature_extractor(x)[0]\n",
    "            elif(self.encoder == 'vit_base_patch16_224_in21k' or self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "                representations = self.feature_extractor(x)\n",
    "            \n",
    "\n",
    "        return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "aug_list = [T.RandomGrayscale(p=0.2), T.RandomHorizontalFlip(),\n",
    "             T.ColorJitter(0.4, 0.4, 0.4, 0.1)]\n",
    "\n",
    "\n",
    "test_img = torch.randn(3,224,224)\n",
    "test_tensor = torch.unsqueeze(test_img, 0)\n",
    "print(test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation:  RandomGrayscale(p=0.2)\n",
      "torch.Size([3, 224, 224])\n",
      "embedding:  torch.Size([1, 768])\n",
      "Augmentation:  RandomHorizontalFlip(p=0.5)\n",
      "torch.Size([3, 224, 224])\n",
      "embedding:  torch.Size([1, 768])\n",
      "Augmentation:  ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[-0.1, 0.1])\n",
      "torch.Size([3, 224, 224])\n",
      "embedding:  torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For each image - \n",
    "1. Generate an augmented version using the augmentation from the list\n",
    "2. Pass this augmented version through the encoder\n",
    "3. Get the embedding and append it to a list\n",
    "4. Get the mean embeddings\n",
    "'''\n",
    "\n",
    "all_embeddings = torch.tensor([])\n",
    "encoder = Encoder(encoder='vit_base_patch32_224_in21k')\n",
    "\n",
    "for aug in aug_list:\n",
    "    print(\"Augmentation: \", aug)\n",
    "    preprocess = T.Compose([T.ToPILImage(), aug, T.ToTensor()])\n",
    "    aug_img = preprocess(test_img)\n",
    "    print(aug_img.size())\n",
    "\n",
    "    embedding = encoder(aug_img.unsqueeze(0))\n",
    "    print('embedding: ', embedding.size())\n",
    "\n",
    "    all_embeddings = torch.cat((all_embeddings,\n",
    "                                embedding), 0)\n",
    "\n",
    "torch.mean(all_embeddings, 0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03444636eb414309a01a2c6bf6ac8f64da93ebb17c812c3812a316112382e716"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pt2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
