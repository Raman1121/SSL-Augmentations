{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import urllib\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pl_bolts.models.self_supervised import SimCLR\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pl_bolts.datasets import DummyDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFAR10('./', download=True,\n",
    "transform=T.Compose([\n",
    "T.ToTensor(),\n",
    "]), train=True)\n",
    "\n",
    "\n",
    "test_set = CIFAR10('./', download=True,\n",
    "transform=T.Compose([\n",
    "T.ToTensor(),\n",
    "]), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DummyDataset((3, 224, 224), (1,), num_samples=100)\n",
    "dl = DataLoader(ds, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to run the model in evaluation mode. For now, implement a single image format where we have a single image and a set of transformations and we calculate the mean embeddings in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(LightningModule):\n",
    "#     def __init__(self, encoder='resnet50_supervised'):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.encoder = encoder\n",
    "\n",
    "#         #TODO: List of encoders in the configuration file\n",
    "#         if encoder not in ['resnet50_supervised', 'simclr_r50', 'vit_base_patch8_224', 'vit_base_patch16_224_in21k', 'vit_base_patch32_224_in21k']:\n",
    "#             raise AssertionError(\"Encoder not in the list of supported encoders.\")\n",
    "        \n",
    "        \n",
    "#         if(self.encoder == 'resnet50_supervised'):\n",
    "#             backbone = models.resnet50(pretrained=True)\n",
    "#             num_filters = backbone.fc.in_features\n",
    "#             layers = list(backbone.children())[:-1]\n",
    "#             self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "#         elif(self.encoder == 'simclr_r50'):\n",
    "#             weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'\n",
    "#             simclr = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "#             self.feature_extractor = simclr.encoder\n",
    "\n",
    "#         elif(self.encoder == 'vit_base_patch16_224_in21k'):\n",
    "#             self.feature_extractor = timm.create_model('vit_base_patch16_224_in21k', pretrained=True, num_classes=0)\n",
    "#             config = resolve_data_config({}, model=self.feature_extractor)\n",
    "#             transform = create_transform(**config)\n",
    "\n",
    "#         elif(self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "#             self.feature_extractor = timm.create_model('vit_base_patch32_224_in21k', pretrained=True, num_classes=0)\n",
    "#             config = resolve_data_config({}, model=self.feature_extractor)\n",
    "#             transform = create_transform(**config)\n",
    "            \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.feature_extractor.eval()\n",
    "#         with torch.no_grad():\n",
    "#             if(self.encoder == 'resnet50_supervised'):\n",
    "#                 representations = self.feature_extractor(x).flatten(1)\n",
    "#             elif(self.encoder == 'simclr_r50'):\n",
    "#                 representations = self.feature_extractor(x)[0]\n",
    "#             elif(self.encoder == 'vit_base_patch16_224_in21k' or self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "#                 representations = self.feature_extractor(x)\n",
    "            \n",
    "\n",
    "#         return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(LightningModule):\n",
    "    def __init__(self, encoder='resnet50_supervised'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "\n",
    "        #TODO: List of encoders in the configuration file\n",
    "        if encoder not in ['resnet50_supervised', 'simclr_r50', 'vit_base_patch16_224_in21k', 'vit_base_patch32_224_in21k']:\n",
    "            raise AssertionError(\"Encoder not in the list of supported encoders.\")\n",
    "        \n",
    "        \n",
    "        if(self.encoder == 'resnet50_supervised'):\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            num_filters = backbone.fc.in_features\n",
    "            layers = list(backbone.children())[:-1]\n",
    "            self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "        elif(self.encoder == 'simclr_r50'):\n",
    "            weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/simclr/bolts_simclr_imagenet/simclr_imagenet.ckpt'\n",
    "            simclr = SimCLR.load_from_checkpoint(weight_path, strict=False)\n",
    "            self.feature_extractor = simclr.encoder\n",
    "\n",
    "        elif(self.encoder == 'vit_base_patch16_224_in21k'):\n",
    "            self.feature_extractor = timm.create_model('vit_base_patch16_224_in21k', pretrained=True, num_classes=0)\n",
    "            config = resolve_data_config({}, model=self.feature_extractor)\n",
    "            transform = create_transform(**config)\n",
    "\n",
    "        elif(self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "            self.feature_extractor = timm.create_model('vit_base_patch32_224_in21k', pretrained=True, num_classes=0)\n",
    "            config = resolve_data_config({}, model=self.feature_extractor)\n",
    "            transform = create_transform(**config)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        #batch_size, channels, height, width = x.size()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if(self.encoder == 'resnet50_supervised'):\n",
    "                representations = self.feature_extractor(x).flatten(1)\n",
    "            elif(self.encoder == 'simclr_r50'):\n",
    "                representations = self.feature_extractor(x)[0]\n",
    "            elif(self.encoder == 'vit_base_patch16_224_in21k' or self.encoder == 'vit_base_patch32_224_in21k'):\n",
    "                representations = self.feature_extractor(x)\n",
    "            \n",
    "\n",
    "        return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "aug_list = [T.RandomGrayscale(p=0.2), T.RandomHorizontalFlip(),\n",
    "             T.ColorJitter(0.4, 0.4, 0.4, 0.1)]                     #This list should be in a configuration file\n",
    "\n",
    "\n",
    "test_img = torch.randn(3,224,224)\n",
    "test_tensor = torch.unsqueeze(test_img, 0)\n",
    "print(test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each image - \n",
    "1. Generate an augmented version using the augmentation from the list\n",
    "2. Pass this augmented version through the encoder\n",
    "3. Get the embedding and append it to a list\n",
    "4. Get the mean embeddings\n",
    "'''\n",
    "\n",
    "# all_embeddings = torch.tensor([])\n",
    "# encoder = Encoder(encoder='vit_base_patch32_224_in21k')\n",
    "\n",
    "# for aug in aug_list:\n",
    "#     print(\"Augmentation: \", aug)\n",
    "#     preprocess = T.Compose([T.ToPILImage(), aug, T.ToTensor()])\n",
    "#     aug_img = preprocess(test_img)\n",
    "#     print(aug_img.size())\n",
    "\n",
    "#     embedding = encoder(aug_img.unsqueeze(0))\n",
    "#     print('embedding: ', embedding.size())\n",
    "\n",
    "#     all_embeddings = torch.cat((all_embeddings,\n",
    "#                                 embedding), 0)\n",
    "\n",
    "# torch.mean(all_embeddings, 0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raman/miniconda3/envs/pt2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "* Feed a dataset of images and obtain embeddings for all images. \n",
    "* Final output for a dataset should have size [num_samples, embedding_size]\n",
    "* 'embedding_size' depends on the encoder used. [ResNet, SimCLR = 2048, ViTs = 768]\n",
    "'''\n",
    "\n",
    "all_embeddings = torch.tensor([])\n",
    "final_dataset_embeddings = torch.tensor([])\n",
    "encoder = Encoder(encoder='vit_base_patch32_224_in21k')\n",
    "\n",
    "index=0\n",
    "for x, y in dl:\n",
    "    \n",
    "    #print(\" ############ BATCH ############\")\n",
    "\n",
    "    #For each image, apply each augmentation\n",
    "    for i in range(x.size()[0]):\n",
    "        all_embed_img = torch.tensor([])    #A tensor to hold embeddings of all augmentations of an image\n",
    "        #print('Image: ', i+1)\n",
    "\n",
    "        for aug in aug_list:\n",
    "            #print(\"Augmentation: \", aug)\n",
    "            preprocess = T.Compose([T.ToPILImage(), aug, T.ToTensor()])\n",
    "            aug_img = preprocess(x[i])\n",
    "            embedding = encoder(aug_img.unsqueeze(0))\n",
    "\n",
    "            all_embed_img = torch.cat((all_embed_img,\n",
    "                                    embedding), 0)\n",
    "\n",
    "        all_embeddings = torch.cat((all_embeddings,\n",
    "                                    torch.mean(all_embed_img, 0)), 0)\n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "final_dataset_embeddings = torch.reshape(all_embeddings, (ds.num_samples, embedding.size()[1]))\n",
    "\n",
    "final_dataset_embeddings.size() #Final size should be [num_samples, embedding_size]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03444636eb414309a01a2c6bf6ac8f64da93ebb17c812c3812a316112382e716"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pt2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
